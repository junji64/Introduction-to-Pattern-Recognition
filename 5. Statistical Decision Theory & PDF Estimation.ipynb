{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통계적 결정이론 및 확률밀도함수 추정\n",
    "\n",
    "## 우도비검증 (Likelihood Ratio Test : LRT)\n",
    "\n",
    "### 데이타의 확률밀도함수를 알 경우의 클래스의 분류\n",
    "\n",
    "임의의 대상체의 측정 (특징 벡터) $x$ 에 주어진 증거에 따라서 분류하는 문제를 생각해보자. (e.g. **측정**:한 사람의 키와 몸무게, **주어진 증거**: 남자와 여자의 키와 몸무게 분포, **분류**: 남자 인지 여자인지 판단)\n",
    "* \"관찰된 특징벡터 $x$ 에 주어진 가장 그럴듯한 클래스로 분류\" 또는\n",
    "* \"각 클래스의 사후확률 $P(\\omega_i|x)$ 을 계산하여 그 중 가장 큰 값을 가지는 클래스로 결정\"\n",
    "\n",
    "여기에서, 사후확률 $P(\\omega_i|x)$ 은 사전확률 $P(\\omega_i)$ 와 우도 $P(x|\\omega_i)$ 의 곱으로 대치하여 계산하며, 이 때 사전확률 $P(\\omega_i)$는 샘플의 수가 $N$ 이라 하고 $\\omega_i$ 에 속하는 샘플 수를 $N_i$ 라 하면, $P(\\omega_i) \\approx N_i/N$ 로 추정하면 된다 (이는, 샘플의 개수가 충분히 크면 실제 확률값에 매우 근접하게 된다.)\n",
    "\n",
    "$$ \\text{if } P(\\omega_1 | x) > P(\\omega_2 | x) \\text{ choose } \\omega_1 \\text{ else choose } \\omega_2$$\n",
    "$$ P(\\omega_1 | x) \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} P(\\omega_2 | x) $$\n",
    "\n",
    "$$ {P(x|\\omega_1) P(\\omega_1) \\over P(x) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} {P(x|\\omega_2) P(\\omega_2) \\over P(x) } $$\n",
    "\n",
    "$$ \\text{ 우도비(Likelihood Ratio) } \\Rightarrow \\Lambda(x) =  {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { P(\\omega_2) \\over  P(\\omega_1)  } $$\n",
    "\n",
    "따라서, 만약에 두 클래스의 사전확률이 같을 경우, $P(\\omega_1) =  P(\\omega_2)$, 특정 클래스로 결정은 우도비 만으로 가능하게 된다. \n",
    "\n",
    "$$ \\text{ 우도비(Likelihood Ratio) } \\Rightarrow \\Lambda(x) =  {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Ex) \n",
    "다음과 같이 두 클래스 $\\omega_1$, $\\omega_2$ 가 주어진 경우 (조건부) 확률밀도함수 (우도함수)가 주어질 경우, LRT 결정 규칙을 유도해 보자. 단, 사전확률은 같다고 가정한다.\n",
    "\n",
    "$$ P(x|\\omega_1) = { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-4)^2}$$\n",
    "$$ P(x|\\omega_2) = { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-10)^2}$$\n",
    "\n",
    "Remember\n",
    "$$  \\Lambda(x) =  {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { P(\\omega_2) \\over  P(\\omega_1)  } $$\n",
    "\n",
    "$$  \\Lambda(x) = { { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-4)^2} \\over { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-10)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1 \\over 1 }$$\n",
    "\n",
    "$$  \\Lambda(x) = {  e^{-{1 \\over 2} (x-4)^2} \\over e^{-{1 \\over 2} (x-10)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1}$$\n",
    "\n",
    "양변에 $\\log()$ 를 취한 후에, $\\log 1=0$ 을 이용하고, 양변에 -2를 곱하면 부등호의 방향이 바뀌게 되어 : \n",
    "\n",
    "$$  (x-4)^2 - (x-10)^2   \\overset{\\omega_1}{\\underset{\\omega_2}{\\lessgtr}} { 0}$$\n",
    "\n",
    "$$  x   \\overset{\\omega_1}{\\underset{\\omega_2}{\\lessgtr}} { 7}$$\n",
    " \n",
    "<img src=\"images/LRT-ex.png\" width=\"300\">\n",
    "\n",
    "* 만약 사전확률이 $P(\\omega_1) = 2 P(\\omega_2)$ 와 같다면, LRT 결정규칙은 어떻게 바뀔까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오류확률(Probability of Error)\n",
    "\n",
    "분류기를 “특징공간을 결정영역으로 분할하는 장치”라고 생각하면, 베이즈 분류기에서 몇 가지 부가적인 통찰을 할 수 있다. 두 개의 클래스가 주어진 경우, 베이즈 분류기를 이용하여 특징공간을 두 영역 $(R_1,R_2)$ 으로 분할할 때, 잘못 분류되는 경우는 다음과 같은 두 가지가 있을 것이다. \n",
    "\n",
    "1. $\\omega_1$에 속하는 특징벡터 $x$를 $R_2$로 결정할 경우, \n",
    "2. $\\omega_2$에 속하는 특징벡터 $x$를 $R_1$로 결정할 경우. \n",
    "\n",
    "오류 사건은 상호 배타적이므로 총 오류 발생 확률은 다음과 같이 각 클래스로부터 생기는 오류확률들을 더한 것으로 표현된다.\n",
    "\n",
    "$$ P[\\text{error}] = \\sum_{i=1}^{2} P[\\text{error} | \\omega_i] P[\\omega_i] $$\n",
    "\n",
    "총 오류확률에서 주어진 각 클래스 내에서 생기는 오류확률은 다음과 같이 표현되고 \n",
    "\n",
    "$$ P[\\text{error} | \\omega_i] =  P[ \\text{choose } \\omega_j | \\omega_i] = \\int_{R_j} P(x|\\omega_i) dx $$\n",
    "\n",
    "따라서 두 클래스에 대한 오류 확률은 다음과 같다.\n",
    "\n",
    "$$ P[\\text{error}] = P[\\omega_1] \\underset{\\epsilon_1}{\\underbrace{\\int_{R_2}P(x | \\omega_1)dx}} +  P[\\omega_2] \\underset{\\epsilon_2}{\\underbrace{\\int_{R_1}P(x | \\omega_2)dx}} $$\n",
    "\n",
    "여기서, $\\epsilon_i$ 는 선택한 $\\omega_j$ 의 영역 $R_j$ 상에서 $P[x|\\omega_i]$ 를 적분한 값이므로 앞에서 살펴본 결정 규칙과 같이 오류확률 $P[\\text{error}]$ 의 사전 확률이 0.5로 같다고 가정하면, 총 오류 확률은 $\\epsilon_1$ 와 $\\epsilon_2$ 의 항으로 다음과 같이 표현된다.\n",
    "\n",
    "$$ P[\\text{error}] = (\\epsilon_1 + \\epsilon_2) / 2 $$\n",
    "\n",
    "<img src=\"images/error-ex.png\" width=\"300\">\n",
    "\n",
    "LRT 결정규칙의 정확도를 알기 위해서, 오류확률을 통한 결정경계의 결정을 고려해 보자. \n",
    "* 어떤 $x$가 주어진 경우의 오류확률 $P[\\text{error}]$은 사후 확률 $P[\\text{error}|x]$로 표현하면, \n",
    "\n",
    "$$ P[\\text{error}] =  \\int_{-\\infty}^{+\\infty} P[\\text{error}|x]P(x) dx $$\n",
    "\n",
    "“최적의 결정경계 = 최소오류확률 “ 이므로 위의 적분이 최소화 되어야 함\n",
    "\n",
    "주어진 점 $x$’에서, $P[\\text{error}|x’]$는 다른 클래스 $\\omega_j$ 를 선택한 경우의 $P[\\omega_i |x’]$로서 다음의 그림과 같이 주어진다.  그림으로부터 어떤 $x’$이던 간에 LRT 결정 규칙이 항상 낮은 $P[\\text{error}|x’]$를 가지며, 따라서 적분을 해도 항상 더 낮은 $P[\\text{error}]$을 갖게 됨을 알 수 있다.\n",
    "\n",
    "<img src=\"images/error-ex2.png\" width=450>\n",
    "\n",
    "\n",
    "어느 문제에서든, **최소 오류 확률은 LRT 결정규칙에 의해서 얻어진다**. 이러한 오류 확률을 **베이즈 오류율(Bayes Error Rate)** 이라고 하며 최고의 분류기 가 된다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이즈 위험(Bayes Risk)\n",
    "베이즈 위험의 의미 : 패턴 분류기가 잘못 분류하여 발생하는 비용($C_{ij}$)의 개념을 베이즈 분류기에 적용한 비용의 기대값, $\\mathscr{R}=E[C]$, ( 여기서 $C_{ij}$ 는 실제로 $\\omega_j$ 인 클래스를 $\\omega_i$ 클래스로 분류했을 때 의 비용을 말함)\n",
    "\n",
    "$$ \\mathscr{R} = E[C] = \\sum_{i=1}^2\\sum_{j=1}^2 C_{ij}\\cdot P[\\text{choose } \\omega_i \\text{ and } x \\in \\omega_j ] = \\sum_{i=1}^2\\sum_{j=1}^2 C_{ij}\\cdot P[x \\in R_i | \\omega_j ] \\cdot P[\\omega_j] $$ \n",
    "\n",
    "베이즈 위험을 최소화하는 결정규칙은 우선 다음과 같은 관계를 이용하면\n",
    "\n",
    "$$  P[x \\in R_i | \\omega_j ] \\cdot P[\\omega_j] = \\int_{R_i} P(x|\\omega_j)dx $$\n",
    "\n",
    "베이즈 위험은 다음과 같이 표현이 된다\n",
    "$$ \\mathscr{R} = \\int_{R_1} \\left [ C_{11}\\cdot P[\\omega_1]\\cdot P(x|\\omega_1) + C_{12}\\cdot P[\\omega_2]\\cdot P(x|\\omega_2) \\right] dx + \\int_{R_2} \\left[ C_{21}\\cdot P[\\omega_1]\\cdot P(x|\\omega_1) + C_{22}\\cdot P[\\omega_2]\\cdot P(x|\\omega_2)\\right] dx$$\n",
    "\n",
    "그리고, 우도(likelihood)를 다음과 같이 표현이 가능하므로\n",
    "$$ \\int_{R_1}  P(x|\\omega_i)dx +  \\int_{R_2}  P(x|\\omega_i)dx =  \\int_{R_1 \\cup  R_2}  P(x|\\omega_i) dx = 1 $$ \n",
    "\n",
    "$$ \\mathscr{R} = C_{11} P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx + \n",
    " C_{12} P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx \\\\\n",
    " +  C_{21} P[\\omega_1] \\int_{R_2} P(x|\\omega_1)dx \n",
    " +  C_{22} P[\\omega_2] \\int_{R_2} P(x|\\omega_2)dx \\\\\n",
    " + C_{21} P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx \n",
    " + C_{22} P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx \\\\\n",
    " - C_{21} P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx \n",
    " - C_{22} P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx $$\n",
    " \n",
    "여기에서, 마지막 두 행은 추가된 항으로, 같은 값을 더하고 빼주었으므로 원래의 식에는 변화가 없게 된다. $R_2$에 대한 적분을 제거하면  (우도에 대한 표현를 이용하여) \n",
    "\n",
    "$$ \\mathscr{R} = C_{21} P[\\omega_1]+C_{22} P[\\omega_2] \\\\\n",
    "+ (C_{12}-C_{22}) P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx\n",
    "- (C_{21}-C_{11}) P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx $$\n",
    "\n",
    "첫 번째 두 항은 상수이므로, 우리는 다음을 최소화하는 결정영역 $R_1$ 을 찾게 된다. \n",
    "\n",
    "$$ R_1 = \\text{argmin} \\{ \\int_{R_1} [ (C_{12}-C_{22}) P[\\omega_2] P(x|\\omega_2)\n",
    "- (C_{21}-C_{11}) P[\\omega_1] P(x|\\omega_1) ]\\}dx\n",
    "\\\\\n",
    "=  \\text{argmin} \\{ \\int_{R_1}g(x) dx \\}$$\n",
    "\n",
    "이제 우리가 찾고 있는 결정영역 $R_1$에 대한 이해를 좀 더 확장하기 위해서 $g(x)$로 치환하여 간략화 한다. 결국 적분  $\\int_{R_1} g(s)dx$ 를 최소화하는 영역 $R_1$을 선택하는 문제로서 $g(x) < 0$ 인 영역을 $R_1$ 에 속하는 영역으로 선택하게 된다.\n",
    "\n",
    "<img src=\"images/BayesRisk.png\" width=350>\n",
    "\n",
    "치환된 g(x) 를 풀면\n",
    "\n",
    "$$ (C_{21} - C_{11}) P[\\omega_1] P(x | \\omega_1) \\overset{\\omega_1}{>} (C_{12} - C_{22}) P[\\omega_2] P(x | \\omega_2) $$\n",
    "\n",
    "이를 다시 정리하면\n",
    "\n",
    "$${P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { (C_{21} - C_{11})P(\\omega_2) \\over (C_{12} - C_{22}) P(\\omega_1)  } $$\n",
    "\n",
    "결국, 베이즈 위험의 최소화를 통해서도 결정 경계를 결정할 수 있으며 우도비 검증 (LRT)이 됨을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Ex) 베이즈 위험을 최소화하는 결정규칙** \n",
    "\n",
    "다음과 같이 두 클래스 우도함수가 주어질 경우, 분류 문제를 고려해보자\n",
    "\n",
    "$$ P(x|\\omega_1) = { 1 \\over \\sqrt{2\\pi} \\sqrt{3}} e ^{-{1 \\over 2}{ x^2 \\over 3}} $$\n",
    "$$ P(x|\\omega_2) = { 1 \\over \\sqrt{2\\pi}} e ^{-{1 \\over 2}(x-2)^2 } $$\n",
    "\n",
    "<img src=\"images/BayesRisk-ex1.png\" width=\"250\">\n",
    "\n",
    "$P[\\omega_1] = P[\\omega_2] = 0.5, C_{11}=C_{22} = 0, C_{12}=1, C_{21}=\\sqrt{3} $ 라고 가정하고, 베이즈위험이 최소화 되는 결정 규칙을 결정하시오.\n",
    "\n",
    "$${P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { (C_{21} - C_{11})P(\\omega_2) \\over (C_{12} - C_{22}) P(\\omega_1)  } $$\n",
    "\n",
    "$$  \\Lambda(x) = { { 1 \\over \\sqrt{2\\pi} \\sqrt{3}} e^{-{1 \\over 2} {x^2 \\over 3}} \\over { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-2)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1 \\over \\sqrt{3} }$$\n",
    "\n",
    "$$  \\Lambda(x) = {  e^{-{1 \\over 2} {x^2 \\over 3}} \\over e^{-{1 \\over 2} (x-2)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1}$$\n",
    "\n",
    "양변에 $\\log()$를 취하면.\n",
    "\n",
    "$$ -{ 1 \\over 2}{ x^2 \\over 3} + { 1 \\over 2} (x-2)^2   \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 0}$$\n",
    "\n",
    "$$ 2x^2 - 12 x + 12    \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 0} \\Rightarrow x = 4.73, 1.27 $$\n",
    "\n",
    "<img src=\"images/BayesRisk-ex2.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LRT 결정규칙의 변형\n",
    "1. 베이즈 위험(Bayes Risk)을 최소화하는 LRT결정규칙을 **Bayes Criterion(베이즈 규준)** 이라고 정의한다.\n",
    "\n",
    "$$\\Lambda(x) = {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { (C_{12} - C_{22})P[\\omega_2] \\over (C_{21} - C_{11}) P[\\omega_1]  } \\Leftarrow \\text{ Bayes criterion} $$\n",
    "\n",
    "2. 대칭적 혹은 비용 값이 0 또는 1인 제로-원 비용함수를 사용하면 베이즈 규준이 사후확률($P(\\omega_i|x)$)의 비로 표현된다. 이 규준을 사후 확률 최대화한다는 의미에서 “**MAP(Maximum A Posteriori) 규준**”이라고 한다.\n",
    "\n",
    "$$ C_{ij} =\n",
    "  \\begin{cases}\n",
    "    0  &  i=j\\\\\n",
    "    1  &  i\\ne j\n",
    "  \\end{cases}\n",
    "  \\Rightarrow \n",
    "\\Lambda(x) = {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { P[\\omega_2] \\over  P[\\omega_1]  } \\Leftrightarrow {P(\\omega_1|x)\\over P(\\omega_2|x) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} {1 } \\Leftarrow\n",
    "\\text{ Maximum A Posteriori (MAP) Criterion} $$\n",
    "\n",
    "3. 사전확률($P(\\omega_i)$)이 같고 제로-원 비용함수의 경우, 베이즈 규준은 우도 $P(x|\\omega_i)$ 의 비로 표현된다. 이 규준을 우도를 최대화한다는 의미에서 **ML(Maximum Likelihood) 규준**이라고 한다.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "C_{ij} =\n",
    " \\begin{cases}\n",
    "    0  &  i=j\\\\\n",
    "    1  &  i\\ne j\n",
    "  \\end{cases} \\\\\n",
    "P(\\omega_i)= {1 \\over C} \\quad \\forall i\n",
    "\\end{cases}\n",
    "\\Rightarrow \n",
    "\\Lambda(x) = {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} 1 \\Leftarrow\n",
    "\\text{ Maximum Likelihood (ML) Criterion} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중클래스결정규칙\n",
    "\n",
    "#### (1) 최소오류확률을 이용한 다중클래스 결정규칙\n",
    "\n",
    "최소오류확률 $P[\\text{error}]$을 최소화하는 결정규칙은 다중클레스 결정문제로 쉽게 일반화 될 수 있다. $P[\\text{error}]=1-P[\\text{correct}]$로부터, “$P[\\text{error}]$를 최소화”하는 문제는“$P[\\text{correct}]$를 최대화”하는 것과 같다. 아래의 식은 $P[\\text{correct}]$를 사후확률의 형태로 표현해 본 결과이다\n",
    "\n",
    "$$ P[\\text{correct}] = \\sum_{i=1}^C P(\\omega_i) \\int_{R_i} P(x|\\omega_i) dx =  \n",
    "\\sum_{i=1}^C \\int_{R_i}  P(x|\\omega_i)  P(\\omega_i) dx =\n",
    "\\sum_{i=1}^C \\int_{R_i}  P(\\omega_i|x)  P(x) dx$$\n",
    "\n",
    "$$\\because \\text{결합확률} : P[B]P[A|B] = P(A\\cap B) = P[A] P[B|A]$$\n",
    "\n",
    "$P[\\text{correct}]$ 를 최대화하기 위해서는 각각의 적분을 최대화해야하며, 각각의 적분은 최대 $P(\\omega_i|x)$를 갖는($p[x]$는 일정하므로) 클래스 $\\omega_i$ 를 선택해야 하고, 따라서 $P(\\omega_i|x)$가 최대가 되는 영역 $R_i$를 정의하게 된다.\n",
    "따라서, $P[\\text{error}]$를 최소화하는 결정규칙은 MAP(Maximum A Posteriori) 규준에 해당한다.\n",
    "\n",
    "<img src=\"images/multi-class-bayes-error.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 베이즈 위험을 이용한 다중 클래스 결정규칙\n",
    "다중 클래스 문제에 대한 베이즈 위험을 최소화하는 결정 규칙은 약간 다른 수식을 사용하여 유도한다. 먼저, 클래스 $\\omega_i$ 를 선택하는 결정을 $\\alpha_i$ 라고 정의하고, 특징 $x$ 를 클래스 $\\omega_i$\n",
    "로 매핑하는 전체 결정규칙을 $\\alpha(x)$ 라고 정의한다. 즉, $\\alpha(x) \\rightarrow \\{ \\alpha_1, \\alpha_2, …, \\alpha_c \\}$. 특징 $x$ 를 클래스  $\\omega_i$  로 할당하는 (조건적) 베이즈 위험 $ R(\\alpha_i|x)$ 은 다음과 같이 표현된다.\n",
    "\n",
    "$$\\mathscr{R}(\\alpha(x) \\rightarrow \\alpha_i) = \\mathscr{R}(\\alpha_i |x) = \\sum_{j=1}^C C_{ij} P (\\omega_j | x) $$\n",
    "\n",
    "그리고 전체 결정 규칙 $\\alpha(x)$과 관련된 베이즈 위험은 다음과 같이 표현된다.\n",
    "\n",
    "$$\\mathscr{R}(\\alpha(x)) = \\int \\mathscr{R}(\\alpha(x) |x) P(x) dx $$\n",
    "\n",
    "이 표현식이 최소화 되기 위해서는 특징공간 상의 각 점 $x$ 에서 베이즈 위험$ R(\\alpha (x)|x)$ 이 최소화 되어야 한다. 즉, $ R(\\alpha_i|x)$ 이 최소인 $\\omega_i$ 를 선택하는 것과 같다.\n",
    "\n",
    "<img src=\"images/multi-bayes-risk.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 판별함수\n",
    "\n",
    "이 장에서 설명한 모든 결정 규칙들은 동일한 구조를 갖고 있다. 이러한 구조는 분류기준함수의 집합 $ g_i(x), i=1,\\cdots,x$ 에 대하여 다음과 같이 정리될 수 있다.\n",
    "\n",
    "만약, $g_i(x) > g_j(x) \\quad \\forall j\\ne i$ 이라면, **특징벡터 $x$를 클래스 $\\omega_i$** 에 속한다고 결정. \n",
    "\n",
    "따라서, $C$ 개의 클래스 중의 하나로 결정하는 시스템은 $C$ 개의 판별함수로 구성된 네트워크로 표현하는 가장 큰 값을 출력하는 카테고리를 선택하는 구조를 가진다.\n",
    "\n",
    "<img src=\"images/discr-func.png\" width=450>\n",
    "\n",
    "\n",
    "| criterion | Discriminant Function |\n",
    "|-----------|-----------------------|\n",
    "| Bayes     | $g_i(x) = -\\mathscr{R}(\\alpha_i | x)$ |\n",
    "| MAP       | $g_i(x) = P(\\omega_i | x)$ |\n",
    "| ML        | $g_i(x) = P(x | \\omega_i) $ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최우추정법에 의한 확률밀도함수의 추정\n",
    "\n",
    "지금까지는 **확률밀도가 주어진 경우**에 영역 결정과 분류기를 어떻게 만드는 지를 설명하였다. 하지만, 대부분의 경우 **실제 확률밀도분포에 대한 지식은 입수 할 수 없으며**, 실험자료로부터 결정되어야 한다. 이 때에는 두 가지 접근법이 일반적이다.\n",
    "\n",
    "1. **파라미타 추정 (Parameter Estimation)법** : 밀도에 대하여 특정한 형태를 가정하여 (e.g.가우시안) 이를 결정짖는 파라미터들 (e.g. 평균과 분산)을 최대우도추정 (Maximum Likelihood Estimation) 방법을 통해서 결정한다.\n",
    "\n",
    "2. **비모수 밀도 추정 (Non-parametric Density Estimation)법** : 밀도에 대하여 어떠한 지식도 가정하지 않는 방법으로, 커널밀도 추정법 또는 최근접이웃 규칙법 등이 있다. \n",
    "\n",
    "**최우추정 (Maximum Likelihood Estimation:MLE)** 이란?: \n",
    "\n",
    "“주어진 자료 $X$를 발생시켰을 가능성이 가장 높은 매개 변수 $\\theta$를 찾아라.”\n",
    "\n",
    "즉, 주어진 자료 $X$에 대해 가장 큰 우도를 갖는 $\\theta$를 찾는 것과 같다.  아래의 그림과 같은 예에서 : \n",
    "\n",
    "$$P(X| \\theta_1) > P(X|\\theta_2)$$\n",
    " \n",
    " \n",
    "<img src=\"./images/ML-1.png\" width=\"300\">\n",
    "\n",
    "최대 우도를 갖는 $\\theta$ 는? 파라미터들이 고정되어 있으나 모른다고 가정한다. 최우추정 해는 다음과 같이 자료 $X$를 가장 잘 설명하는 해를 구한다.\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[p(X|\\theta)] $$\n",
    " \n",
    "<img src=\"./images/ML-2.jpg\" width=\"250\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최대 우도 추정 (최우추정, MLE : Maximum Likelihood Estimation)**\n",
    "\n",
    "일련의 파라미터들 $\\theta=[\\theta_1, \\theta_2, …, \\theta_N]^T$ 으로 구성된 어떤 확률밀도함수 $p(x|\\theta)$ 로부터 관측된 표본 데이터 집합을 $X={x^{(1},x^{(2},…,x^{(N}}$ 라 할 때, \n",
    "이 표본들로부터 파라미터 $\\theta=[\\theta_1, \\theta_2, …, \\theta_N]^T$  들을 추정하는 문제를 생각해보자. (즉, 평균, $\\mu_0$, 와 분산,$\\sigma_0^2$ 를 갖는 가우시안 확률분포로부터 관측된 표본데이터로부터 평균과 분산을 추정하는 문제). 어떤 표본 집합이 특정한 확률밀도 함수 $p(x|\\theta)$ 로 표현되는 프로세스로부터 발생한 데이터로 이루어져 있다면, 전체 표본 집합은 결합확률밀도로 다음과 같이 표현된다.\n",
    "\n",
    "$$ p(X|\\theta) = p(x^{(1} | \\theta) p(x^{(2} | \\theta) \\cdots p(x^{(N} | \\theta) = \\prod_{k=1}^N p(x^{(k} | \\theta) $$\n",
    "\n",
    "그런데, 이 함수는 확률함수이므로 가장 큰 확률 값을 발생시키는 $\\theta$ 값을 추정값  $\\hat{\\theta}$    으로 보는 것이 가장 그럴듯할 것이다. 이는 자료를 가장 그렇게 줄 수 있는 θ 값을 얻는 직관적인 생각에 해당한다. 여기서, $p(x|\\theta)$ 는 파라미터 $\\theta$  에 따르는 주어진 데이터 집합의 우도(likelihood) 함수이다. 우도 함수의 곱을 합으로 하기 위해서 로그를 취하면 다음과 같이 된다.\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[p(X|\\theta)] = \\arg\\max[\\log p(X|\\theta)]$$\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[\\log \\prod_{k-1}^N p(x^{(k}|\\theta)] = \\arg\\max[ \\sum_{k-1}^N \\log p(x^{(k}|\\theta)]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**로그-우도를 사용하는 이유**\n",
    "\n",
    "분석을 위해서는 우도의 로그를 갖고 일하는 것이 편리하다. 로그는 단조함수(monotonic function: 단조증가함수와 단조감소함수의 총칭 ) 이므로\n",
    "\n",
    "<img src=\"images/log-ML.png\" width=\"550\">\n",
    "\n",
    "로그를 취하면 파라미터의 최우추정은 다음과 같이 되어\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[\\log \\prod_{k-1}^N p(x^{(k}|\\theta)] = \\arg\\max[ \\sum_{k-1}^N \\log p(x^{(k}|\\theta)]  $$\n",
    "\n",
    "\n",
    "* 항들의 합을 최대화 시키는 것은 곱을 최대화 시키는 것보다 쉬운 일이며  (항들의 곱들에 대한 미분을 생각해 보면 알 수 있음), \n",
    "* 분포가 가우시안이면 로그를 취하는 것의 장점이 더 명확해진다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단변량 가우시안의 경우**: \n",
    "\n",
    "확률밀도함수가 $p(x) = \\mathscr{N}(\\mu, \\sigma)$ 로 주어진 자료 $X=\\{x^{(1},x^{(2},…,x^{(N}\\}$ 에서 표준편차 $\\sigma$ 가 주어진 경우, 평균의 최우추정은?\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta} & = \\arg\\max \\sum_{k=1}^N \\log p(x^{(k} | \\theta) \\\\\n",
    "             & = \\arg\\max \\sum_{k=1}^N \\log \\left( {1 \\over \\sqrt{2\\pi} \\sigma} \\exp \\left( - {1 \\over 2 \\sigma^2} (x^{(k} - \\mu)^2 \\right)  \\right) \\\\\n",
    "             & = \\arg\\max \\sum_{k=1}^N \\left( \\log \\left( {1 \\over \\sqrt{2\\pi} \\sigma} \\right) - {1 \\over 2 \\sigma^2} (x^{(k} - \\mu)^2  \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "함수의 최대(또는 최소)는 미분이 0 이 되는 곳으로 정의되므로,\n",
    "\n",
    "$$ {\\partial \\over \\partial \\mu} \\sum_{k=1}^N \\left( \\log \\left( {1 \\over \\sqrt{2\\pi} \\sigma} \\right) - {1 \\over 2 \\sigma^2} (x^{(k} - \\mu)^2 \\right)$$\n",
    "\n",
    "$$ = - {1 \\over 2 \\sigma^2} {\\partial \\over \\partial \\mu} \\sum_{k=1}^N \\left( x^{2(k} -2x^{(k}\\mu + \\mu^2 \\right)  = {1 \\over 2 \\sigma^2} {\\partial \\over \\partial \\mu} \\sum_{k=1}^N \\left( 2x^{(k}\\mu - \\mu^2 \\right) = 0 $$\n",
    "$$ \\Rightarrow  \\sum_{k=1}^N \\left( x^{(k} - \\mu \\right) = \\sum_{k=1}^N x^{(k} -N \\mu = 0 \\Rightarrow \\mu = {1 \\over N} \\sum_{k=1}^N x^{(k}  $$\n",
    "\n",
    "따라서 가우시안 확률밀도의 평균의 최우추정은 훈련자료의 산술 평균(매우 직관적인 결과)이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률밀도함수가 $ p(x) = \\mathscr{N}(\\mu, \\sigma)$ 로 주어진 자료  $X=\\{x^{(1},x^{(2},…,x^{(N}\\}$  에서 표준편차(σ) 와 평균(μ)의 최우추정은 변수가 두 개이므로 미분은 그래디언트가 된다.\n",
    "\n",
    "$$ \\hat{\\theta} = \\begin{bmatrix} \\theta_1 = \\mu \\\\ \\theta_2 = \\sigma^2 \\end{bmatrix}\n",
    "\\Rightarrow \\nabla_\\theta = \\begin{bmatrix} {\\partial \\over \\partial \\theta_1} \\sum_{k=1}^N \\log p(x^{(k} | \\theta) \\\\  {\\partial \\over \\partial \\theta_2} \\sum_{k=1}^N \\log p(x^{(k} | \\theta) \\end{bmatrix}\n",
    "= \\sum_{k=1}^N \\begin{bmatrix} {1 \\over \\theta_2}(x^{(k} - \\theta_1)  \\\\ -{1 \\over 2\\theta_2} + {(x^{(k} - \\theta_1)^2 \\over 2 \\theta_2^2 } \\end{bmatrix} = 0 $$\n",
    "\n",
    "\n",
    "$\\theta_1$ 과 $\\theta_2$ 에 대하여 풀면\n",
    "\n",
    "\n",
    "$$ \\hat{\\theta}_1 = { 1 \\over N} \\sum_{k=1}^N \\mathbf{x}^{(k};  \\quad \\hat{\\theta}_2 = { 1 \\over N} \\sum_{k=1}^N (x^{(k} - \\hat{\\theta}_1)^2$$\n",
    "\n",
    "따라서 분산의 최우추정은 훈련자료의 표본분산이 되며, \n",
    "\n",
    "**다변량 가우시안의 최우추정**은 **표본평균벡터**와 **표본공분산행렬**이 됨도 유사한 방법으로 보일 수 있다. \n",
    "\n",
    "\n",
    "$$ \\hat{\\mathbf{\\mu}} = { 1 \\over N} \\sum_{k=1}^N x^{(k};  \\quad \\hat{\\Sigma} = { 1 \\over N} \\sum_{k=1}^N (x^{(k} - \\hat{\\mu}) (x^{(k} - \\hat{\\mu})^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최우추정법에 의한 확률밀도함수의 추정\n",
    "\n",
    "이러한 추정이 얼마나 잘 된 것인지를 어떻게 알 수 있나? 통계학적 추정의 정확도를 말해주는 두 측정치 bias 와 Variance\n",
    "\n",
    "* Bias : 추정치가 실제 값에 얼마나 가까운가를 말하는 정도\n",
    "* Variance : 자료가 다를 경우 추정치가 얼마나 변화하는가를 나타내는 수치.\n",
    "\n",
    "<img src=\"images/bias-variance.png\" width=\"250\">\n",
    "\n",
    "Bias-Variance tradeoff : 하나를 감소시키기 위해서는 다른 하나를 희생해야만 함\n",
    "\n",
    "<img src=\"images/bias-variance-2.png\" width=\"450\">\n",
    "\n",
    "* **평균의 최우추정은 언바이어스드(unbiased) 추정**이다.!\n",
    "\n",
    "$$ E[\\hat{\\mu}] = E \\left[ { 1 \\over N} \\sum_{k=1}^N x_k \\right] = { 1 \\over N} \\sum_{k=1}^N E[x_k] = \\mu$$\n",
    "\n",
    "* **분산의 최우추정은 바이어스드(biased) 추정**이다. !\n",
    "\n",
    "$$ E[\\hat{\\sigma}^2] = E \\left[ { 1 \\over N} \\sum_{k=1}^N (x_k -\\hat{\\mu})^2 \\right] = \\cdots =\n",
    "{ N-1 \\over N} Var(x_k) \\ne Var(x_k)$$\n",
    "\n",
    "이는 분산의 최우추정에 진짜 평균이 아닌 최우추정 평균이 사용되기 때문이다.\n",
    "하지만 $N$ 이 무한이 증가하면, 바이어스는 제로(0)에 가까워진다. \n",
    "따라서 때때로 다음과 같은 언바이어스드 공분산을 사용한다. **다변량의 경우**에는 다음과 같이 된다.\n",
    "\n",
    "$$  \\hat{\\Sigma}_{\\text{unbiased}} = { 1 \\over N-1} \\sum_{k=1}^N (x^{(k} - \\hat{\\mu}) (x^{(k} - \\hat{\\mu})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최우추정과 우도비검증 판별함수 시뮬레이션\n",
    "\n",
    "가우시안 분포를 이루는 데이터에 대하여 **최우추정(MLE)법**으로 우도함수의 파라미터를 추정하고, **우도비검증(LRT) 판별함수**를 이용하여 미지의 패턴을 인식하는 실습을 해보자.\n",
    "\n",
    "1. 가우시안 분포로 2-클래스를 이루는 임의의 데이터 집합을 생성한다.\n",
    "2. 주어진 데이터에서 확률밀도함수를 MLE법으로 추정한다.\n",
    "3. 추정된 확률밀도함수가 원래 확률밀도함수에 얼마나 근접한지를 확인한다.\n",
    "4. 임의의 데이터가 주어질 경우에, 이 데이터가 어느 클레스에 속하는지를 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[380, 400] [381.15846887 401.06700731]\n",
      "[[200, 0], [0, 200]] [[177.25763058  12.55133872]\n",
      " [ 12.55133872 206.37080182]]\n",
      "[430, 350] [428.75103193 348.66290567]\n",
      "[[240, 150], [150, 240]] [[238.45947632 143.53494869]\n",
      " [143.53494869 248.53655513]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mu1 = [380, 400]\n",
    "cov1 = [[200, 0], [0, 200]]\n",
    "rv1 = sp.stats.multivariate_normal(mu1, cov1)\n",
    "X1 = rv1.rvs(200)\n",
    "plt.scatter(X1[:, 0], X1[:, 1], s=1)\n",
    "\n",
    "mu2 = [430, 350]\n",
    "cov2 = [[240,150], [150, 240]]\n",
    "rv2 = sp.stats.multivariate_normal(mu2, cov2)\n",
    "X2 = rv2.rvs(200)\n",
    "plt.scatter(X2[:, 0], X2[:, 1], s=2)\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "mu1_ = X1.mean(axis=0)\n",
    "cov1_ = np.cov(X1.T, ddof=0)\n",
    "print(mu1, mu1_)\n",
    "plt.scatter(mu1[0], mu1[1], s=60, marker='o')\n",
    "plt.scatter(mu1_[0], mu1_[1], s=60, marker='x')\n",
    "print(cov1,cov1_)\n",
    "\n",
    "mu2_ = X2.mean(axis=0)\n",
    "cov2_ = np.cov(X2.T, ddof=0)\n",
    "print(mu2, mu2_)\n",
    "plt.scatter(mu2[0], mu2[1], s=60, marker='o')\n",
    "plt.scatter(mu2_[0], mu2_[1], s=60, marker='x')\n",
    "print(cov2, cov2_)\n",
    "\n",
    "test_1 = [390, 380]\n",
    "plt.scatter(test_1[0], test_1[1], s=40, marker='^')\n",
    "test_2 = [420, 360]\n",
    "plt.scatter(test_2[0], test_2[1], s=40, marker='^')\n",
    "test_3 = [430, 370]\n",
    "plt.scatter(test_3[0], test_3[1], s=40, marker='^')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[390, 380]  is class_1\n",
      "[420, 360]  is class_2\n",
      "[430, 370]  is class_2\n"
     ]
    }
   ],
   "source": [
    "result_1_1 = sp.stats.multivariate_normal.pdf(test_1, mu1_, cov1_);\n",
    "result_1_2 = sp.stats.multivariate_normal.pdf(test_1, mu2_, cov2_);\n",
    "decision = \"class_1\" if result_1_1 > result_1_2 else \"class_2\"\n",
    "print( test_1,\" is\",decision )\n",
    "result_2_1 = sp.stats.multivariate_normal.pdf(test_2, mu1_, cov1_);\n",
    "result_2_2 = sp.stats.multivariate_normal.pdf(test_2, mu2_, cov2_);\n",
    "decision = \"class_1\" if result_2_1 > result_2_2 else \"class_2\"\n",
    "print( test_2,\" is\",decision )\n",
    "result_3_1 = sp.stats.multivariate_normal.pdf(test_3, mu1_, cov1_);\n",
    "result_3_2 = sp.stats.multivariate_normal.pdf(test_3, mu2_, cov2_);\n",
    "decision = \"class_1\" if result_3_1 > result_3_2 else \"class_2\"\n",
    "print(test_3,\" is\",decision )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
