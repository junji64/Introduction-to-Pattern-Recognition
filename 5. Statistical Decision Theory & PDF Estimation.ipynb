{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 통계적 결정이론 및 확률밀도함수 추정\n",
    "\n",
    "## 우도비검증 (Likelihood Ratio Test : LRT)\n",
    "\n",
    "### 데이타의 확률밀도함수를 알 경우의 클래스의 분류\n",
    "\n",
    "임의의 대상체의 측정 (특징 벡터) $x$ 에 주어진 증거에 따라서 분류하는 문제를 생각해보자. (e.g. **측정**:한 사람의 키와 몸무게, **주어진 증거**: 남자와 여자의 키와 몸무게 분포, **분류**: 남자 인지 여자인지 판단)\n",
    "* \"관찰된 특징벡터 $x$ 에 주어진 가장 그럴듯한 클래스로 분류\" 또는\n",
    "* \"각 클래스의 사후확률 $P(\\omega_i|x)$ 을 계산하여 그 중 가장 큰 값을 가지는 클래스로 결정\"\n",
    "\n",
    "여기에서, 사후확률 $P(\\omega_i|x)$ 은 사전확률 $P(\\omega_i)$ 와 우도 $P(x|\\omega_i)$ 의 곱으로 대치하여 계산하며, 이 때 사전확률 $P(\\omega_i)$는 샘플의 수가 $N$ 이라 하고 $\\omega_i$ 에 속하는 샘플 수를 $N_i$ 라 하면, $P(\\omega_i) \\approx N_i/N$ 로 추정하면 된다 (이는, 샘플의 개수가 충분히 크면 실제 확률값에 매우 근접하게 된다.)\n",
    "\n",
    "$$ \\text{if } P(\\omega_1 | x) > P(\\omega_2 | x) \\text{ choose } \\omega_1 \\text{ else choose } \\omega_2$$\n",
    "$$ P(\\omega_1 | x) \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} P(\\omega_2 | x) $$\n",
    "\n",
    "$$ {P(x|\\omega_1) P(\\omega_1) \\over P(x) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} {P(x|\\omega_2) P(\\omega_2) \\over P(x) } $$\n",
    "\n",
    "$$ \\text{ 우도비(Likelihood Ratio) } \\Rightarrow \\Lambda(x) =  {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { P(\\omega_2) \\over  P(\\omega_1)  } $$\n",
    "\n",
    "따라서, 만약에 두 클래스의 사전확률이 같을 경우, $P(\\omega_1) =  P(\\omega_2)$, 특정 클래스로 결정은 우도비 만으로 가능하게 된다. \n",
    "\n",
    "$$ \\text{ 우도비(Likelihood Ratio) } \\Rightarrow \\Lambda(x) =  {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (Ex) \n",
    "다음과 같이 두 클래스 $\\omega_1$, $\\omega_2$ 가 주어진 경우 (조건부) 확률밀도함수 (우도함수)가 주어질 경우, LRT 결정 규칙을 유도해 보자. 단, 사전확률은 같다고 가정한다.\n",
    "\n",
    "$$ P(x|\\omega_1) = { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-4)^2}$$\n",
    "$$ P(x|\\omega_2) = { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-10)^2}$$\n",
    "\n",
    "Remember\n",
    "$$  \\Lambda(x) =  {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { P(\\omega_2) \\over  P(\\omega_1)  } $$\n",
    "\n",
    "$$  \\Lambda(x) = { { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-4)^2} \\over { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-10)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1 \\over 1 }$$\n",
    "\n",
    "$$  \\Lambda(x) = {  e^{-{1 \\over 2} (x-4)^2} \\over e^{-{1 \\over 2} (x-10)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1}$$\n",
    "\n",
    "양변에 $\\log()$ 를 취한 후에, $\\log 1=0$ 을 이용하고, 양변에 -2를 곱하면 부등호의 방향이 바뀌게 되어 : \n",
    "\n",
    "$$  (x-4)^2 - (x-10)^2   \\overset{\\omega_1}{\\underset{\\omega_2}{\\lessgtr}} { 0}$$\n",
    "\n",
    "$$  x   \\overset{\\omega_1}{\\underset{\\omega_2}{\\lessgtr}} { 7}$$\n",
    " \n",
    "<img src=\"images/LRT-ex.png\" width=\"300\">\n",
    "\n",
    "* 만약 사전확률이 $P(\\omega_1) = 2 P(\\omega_2)$ 와 같다면, LRT 결정규칙은 어떻게 바뀔까?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 오류확률(Probability of Error)\n",
    "\n",
    "분류기를 “특징공간을 결정영역으로 분할하는 장치”라고 생각하면, 베이즈 분류기에서 몇 가지 부가적인 통찰을 할 수 있다. 두 개의 클래스가 주어진 경우, 베이즈 분류기를 이용하여 특징공간을 두 영역 $(R_1,R_2)$ 으로 분할할 때, 잘못 분류되는 경우는 다음과 같은 두 가지가 있을 것이다. \n",
    "\n",
    "1. $\\omega_1$에 속하는 특징벡터 $x$를 $R_2$로 결정할 경우, \n",
    "2. $\\omega_2$에 속하는 특징벡터 $x$를 $R_1$로 결정할 경우. \n",
    "\n",
    "오류 사건은 상호 배타적이므로 총 오류 발생 확률은 다음과 같이 각 클래스로부터 생기는 오류확률들을 더한 것으로 표현된다.\n",
    "\n",
    "$$ P[\\text{error}] = \\sum_{i=1}^{2} P[\\text{error} | \\omega_i] P[\\omega_i] $$\n",
    "\n",
    "총 오류확률에서 주어진 각 클래스 내에서 생기는 오류확률은 다음과 같이 표현되고 \n",
    "\n",
    "$$ P[\\text{error} | \\omega_i] =  P[ \\text{choose } \\omega_j | \\omega_i] = \\int_{R_j} P(x|\\omega_i) dx $$\n",
    "\n",
    "따라서 두 클래스에 대한 오류 확률은 다음과 같다.\n",
    "\n",
    "$$ P[\\text{error}] = P[\\omega_1] \\underset{\\epsilon_1}{\\underbrace{\\int_{R_2}P(x | \\omega_1)dx}} +  P[\\omega_2] \\underset{\\epsilon_2}{\\underbrace{\\int_{R_1}P(x | \\omega_2)dx}} $$\n",
    "\n",
    "여기서, $\\epsilon_i$ 는 선택한 $\\omega_j$ 의 영역 $R_j$ 상에서 $P[x|\\omega_i]$ 를 적분한 값이므로 앞에서 살펴본 결정 규칙과 같이 오류확률 $P[\\text{error}]$ 의 사전 확률이 0.5로 같다고 가정하면, 총 오류 확률은 $\\epsilon_1$ 와 $\\epsilon_2$ 의 항으로 다음과 같이 표현된다.\n",
    "\n",
    "$$ P[\\text{error}] = (\\epsilon_1 + \\epsilon_2) / 2 $$\n",
    "\n",
    "<img src=\"images/error-ex.png\" width=\"300\">\n",
    "\n",
    "LRT 결정규칙의 정확도를 알기 위해서, 오류확률을 통한 결정경계의 결정을 고려해 보자. \n",
    "* 어떤 $x$가 주어진 경우의 오류확률 $P[\\text{error}]$은 사후 확률 $P[\\text{error}|x]$로 표현하면, \n",
    "\n",
    "$$ P[\\text{error}] =  \\int_{-\\infty}^{+\\infty} P[\\text{error}|x]P(x) dx $$\n",
    "\n",
    "“최적의 결정경계 = 최소오류확률 “ 이므로 위의 적분이 최소화 되어야 함\n",
    "\n",
    "주어진 점 $x$’에서, $P[\\text{error}|x’]$는 다른 클래스 $\\omega_j$ 를 선택한 경우의 $P[\\omega_i |x’]$로서 다음의 그림과 같이 주어진다.  그림으로부터 어떤 $x’$이던 간에 LRT 결정 규칙이 항상 낮은 $P[\\text{error}|x’]$를 가지며, 따라서 적분을 해도 항상 더 낮은 $P[\\text{error}]$을 갖게 됨을 알 수 있다.\n",
    "\n",
    "<img src=\"images/error-ex2.png\" width=450>\n",
    "\n",
    "\n",
    "어느 문제에서든, **최소 오류 확률은 LRT 결정규칙에 의해서 얻어진다**. 이러한 오류 확률을 **베이즈 오류율(Bayes Error Rate)** 이라고 하며 최고의 분류기 가 된다. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 베이즈 위험(Bayes Risk)\n",
    "베이즈 위험의 의미 : 패턴 분류기가 잘못 분류하여 발생하는 비용($C_{ij}$)의 개념을 베이즈 분류기에 적용한 비용의 기대값, $\\mathscr{R}=E[C]$, ( 여기서 $C_{ij}$ 는 실제로 $\\omega_j$ 인 클래스를 $\\omega_i$ 클래스로 분류했을 때 의 비용을 말함)\n",
    "\n",
    "$$ \\mathscr{R} = E[C] = \\sum_{i=1}^2\\sum_{j=1}^2 C_{ij}\\cdot P[\\text{choose } \\omega_i \\text{ and } x \\in \\omega_j ] = \\sum_{i=1}^2\\sum_{j=1}^2 C_{ij}\\cdot P[x \\in R_i | \\omega_j ] \\cdot P[\\omega_j] $$ \n",
    "\n",
    "베이즈 위험을 최소화하는 결정규칙은 우선 다음과 같은 관계를 이용하면\n",
    "\n",
    "$$  P[x \\in R_i | \\omega_j ] \\cdot P[\\omega_j] = \\int_{R_i} P(x|\\omega_j)dx $$\n",
    "\n",
    "베이즈 위험은 다음과 같이 표현이 된다\n",
    "$$ \\mathscr{R} = \\int_{R_1} \\left [ C_{11}\\cdot P[\\omega_1]\\cdot P(x|\\omega_1) + C_{12}\\cdot P[\\omega_2]\\cdot P(x|\\omega_2) \\right] dx + \\int_{R_2} \\left[ C_{21}\\cdot P[\\omega_1]\\cdot P(x|\\omega_1) + C_{22}\\cdot P[\\omega_2]\\cdot P(x|\\omega_2)\\right] dx$$\n",
    "\n",
    "그리고, 우도(likelihood)를 다음과 같이 표현이 가능하므로\n",
    "$$ \\int_{R_1}  P(x|\\omega_i)dx +  \\int_{R_2}  P(x|\\omega_i)dx =  \\int_{R_1 \\cup  R_2}  P(x|\\omega_i) dx = 1 $$ \n",
    "\n",
    "$$ \\mathscr{R} = C_{11} P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx + \n",
    " C_{12} P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx \\\\\n",
    " +  C_{21} P[\\omega_1] \\int_{R_2} P(x|\\omega_1)dx \n",
    " +  C_{22} P[\\omega_2] \\int_{R_2} P(x|\\omega_2)dx \\\\\n",
    " + C_{21} P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx \n",
    " + C_{22} P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx \\\\\n",
    " - C_{21} P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx \n",
    " - C_{22} P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx $$\n",
    " \n",
    "여기에서, 마지막 두 행은 추가된 항으로, 같은 값을 더하고 빼주었으므로 원래의 식에는 변화가 없게 된다. $R_2$에 대한 적분을 제거하면  (우도에 대한 표현를 이용하여) \n",
    "\n",
    "$$ \\mathscr{R} = C_{21} P[\\omega_1]+C_{22} P[\\omega_2] \\\\\n",
    "+ (C_{12}-C_{22}) P[\\omega_2] \\int_{R_1} P(x|\\omega_2)dx\n",
    "- (C_{21}-C_{11}) P[\\omega_1] \\int_{R_1} P(x|\\omega_1)dx $$\n",
    "\n",
    "첫 번째 두 항은 상수이므로, 우리는 다음을 최소화하는 결정영역 $R_1$ 을 찾게 된다. \n",
    "\n",
    "$$ R_1 = \\text{argmin} \\{ \\int_{R_1} [ (C_{12}-C_{22}) P[\\omega_2] P(x|\\omega_2)\n",
    "- (C_{21}-C_{11}) P[\\omega_1] P(x|\\omega_1) ]\\}dx\n",
    "\\\\\n",
    "=  \\text{argmin} \\{ \\int_{R_1}g(x) dx \\}$$\n",
    "\n",
    "이제 우리가 찾고 있는 결정영역 $R_1$에 대한 이해를 좀 더 확장하기 위해서 $g(x)$로 치환하여 간략화 한다. 결국 적분  $\\int_{R_1} g(s)dx$ 를 최소화하는 영역 $R_1$을 선택하는 문제로서 $g(x) < 0$ 인 영역을 $R_1$ 에 속하는 영역으로 선택하게 된다.\n",
    "\n",
    "<img src=\"images/BayesRisk.png\" width=350>\n",
    "\n",
    "치환된 g(x) 를 풀면\n",
    "\n",
    "$$ (C_{21} - C_{11}) P[\\omega_1] P(x | \\omega_1) \\overset{\\omega_1}{>} (C_{12} - C_{22}) P[\\omega_2] P(x | \\omega_2) $$\n",
    "\n",
    "이를 다시 정리하면\n",
    "\n",
    "$${P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { (C_{21} - C_{11})P(\\omega_2) \\over (C_{12} - C_{22}) P(\\omega_1)  } $$\n",
    "\n",
    "결국, 베이즈 위험의 최소화를 통해서도 결정 경계를 결정할 수 있으며 우도비 검증 (LRT)이 됨을 알 수 있다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Ex) 베이즈 위험을 최소화하는 결정규칙** \n",
    "\n",
    "다음과 같이 두 클래스 우도함수가 주어질 경우, 분류 문제를 고려해보자\n",
    "\n",
    "$$ P(x|\\omega_1) = { 1 \\over \\sqrt{2\\pi} \\sqrt{3}} e ^{-{1 \\over 2}{ x^2 \\over 3}} $$\n",
    "$$ P(x|\\omega_2) = { 1 \\over \\sqrt{2\\pi}} e ^{-{1 \\over 2}(x-2)^2 } $$\n",
    "\n",
    "<img src=\"images/BayesRisk-ex1.png\" width=\"250\">\n",
    "\n",
    "$P[\\omega_1] = P[\\omega_2] = 0.5, C_{11}=C_{22} = 0, C_{12}=1, C_{21}=\\sqrt{3} $ 라고 가정하고, 베이즈위험이 최소화 되는 결정 규칙을 결정하시오.\n",
    "\n",
    "$${P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { (C_{21} - C_{11})P(\\omega_2) \\over (C_{12} - C_{22}) P(\\omega_1)  } $$\n",
    "\n",
    "$$  \\Lambda(x) = { { 1 \\over \\sqrt{2\\pi} \\sqrt{3}} e^{-{1 \\over 2} {x^2 \\over 3}} \\over { 1 \\over \\sqrt{2\\pi}} e^{-{1 \\over 2} (x-2)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1 \\over \\sqrt{3} }$$\n",
    "\n",
    "$$  \\Lambda(x) = {  e^{-{1 \\over 2} {x^2 \\over 3}} \\over e^{-{1 \\over 2} (x-2)^2} }  \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 1}$$\n",
    "\n",
    "양변에 $\\log()$를 취하면.\n",
    "\n",
    "$$ -{ 1 \\over 2}{ x^2 \\over 3} + { 1 \\over 2} (x-2)^2   \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 0}$$\n",
    "\n",
    "$$ 2x^2 - 12 x + 12    \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { 0} \\Rightarrow x = 4.73, 1.27 $$\n",
    "\n",
    "<img src=\"images/BayesRisk-ex2.png\" width=\"250\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LRT 결정규칙의 변형\n",
    "1. 베이즈 위험(Bayes Risk)을 최소화하는 LRT결정규칙을 **Bayes Criterion(베이즈 규준)** 이라고 정의한다.\n",
    "\n",
    "$$\\Lambda(x) = {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { (C_{12} - C_{22})P[\\omega_2] \\over (C_{21} - C_{11}) P[\\omega_1]  } \\Leftarrow \\text{ Bayes criterion} $$\n",
    "\n",
    "2. 대칭적 혹은 비용 값이 0 또는 1인 제로-원 비용함수를 사용하면 베이즈 규준이 사후확률($P(\\omega_i|x)$)의 비로 표현된다. 이 규준을 사후 확률 최대화한다는 의미에서 “**MAP(Maximum A Posteriori) 규준**”이라고 한다.\n",
    "\n",
    "$$ C_{ij} =\n",
    "  \\begin{cases}\n",
    "    0  &  i=j\\\\\n",
    "    1  &  i\\ne j\n",
    "  \\end{cases}\n",
    "  \\Rightarrow \n",
    "\\Lambda(x) = {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} { P[\\omega_2] \\over  P[\\omega_1]  } \\Leftrightarrow {P(\\omega_1|x)\\over P(\\omega_2|x) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} {1 } \\Leftarrow\n",
    "\\text{ Maximum A Posteriori (MAP) Criterion} $$\n",
    "\n",
    "3. 사전확률($P(\\omega_i)$)이 같고 제로-원 비용함수의 경우, 베이즈 규준은 우도 $P(x|\\omega_i)$ 의 비로 표현된다. 이 규준을 우도를 최대화한다는 의미에서 **ML(Maximum Likelihood) 규준**이라고 한다.\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "C_{ij} =\n",
    " \\begin{cases}\n",
    "    0  &  i=j\\\\\n",
    "    1  &  i\\ne j\n",
    "  \\end{cases} \\\\\n",
    "P(\\omega_i)= {1 \\over C} \\quad \\forall i\n",
    "\\end{cases}\n",
    "\\Rightarrow \n",
    "\\Lambda(x) = {P(x|\\omega_1)\\over P(x|\\omega_2) } \\overset{\\omega_1}{\\underset{\\omega_2}{\\gtrless}} 1 \\Leftarrow\n",
    "\\text{ Maximum Likelihood (ML) Criterion} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 다중클래스결정규칙\n",
    "\n",
    "#### (1) 최소오류확률을 이용한 다중클래스 결정규칙\n",
    "\n",
    "최소오류확률 $P[\\text{error}]$을 최소화하는 결정규칙은 다중클레스 결정문제로 쉽게 일반화 될 수 있다. $P[\\text{error}]=1-P[\\text{correct}]$로부터, “$P[\\text{error}]$를 최소화”하는 문제는“$P[\\text{correct}]$를 최대화”하는 것과 같다. 아래의 식은 $P[\\text{correct}]$를 사후확률의 형태로 표현해 본 결과이다\n",
    "\n",
    "$$ P[\\text{correct}] = \\sum_{i=1}^C P(\\omega_i) \\int_{R_i} P(x|\\omega_i) dx =  \n",
    "\\sum_{i=1}^C \\int_{R_i}  P(x|\\omega_i)  P(\\omega_i) dx =\n",
    "\\sum_{i=1}^C \\int_{R_i}  P(\\omega_i|x)  P(x) dx$$\n",
    "\n",
    "$$\\because \\text{결합확률} : P[B]P[A|B] = P(A\\cap B) = P[A] P[B|A]$$\n",
    "\n",
    "$P[\\text{correct}]$ 를 최대화하기 위해서는 각각의 적분을 최대화해야하며, 각각의 적분은 최대 $P(\\omega_i|x)$를 갖는($p[x]$는 일정하므로) 클래스 $\\omega_i$ 를 선택해야 하고, 따라서 $P(\\omega_i|x)$가 최대가 되는 영역 $R_i$를 정의하게 된다.\n",
    "따라서, $P[\\text{error}]$를 최소화하는 결정규칙은 MAP(Maximum A Posteriori) 규준에 해당한다.\n",
    "\n",
    "<img src=\"images/multi-class-bayes-error.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) 베이즈 위험을 이용한 다중 클래스 결정규칙\n",
    "다중 클래스 문제에 대한 베이즈 위험을 최소화하는 결정 규칙은 약간 다른 수식을 사용하여 유도한다. 먼저, 클래스 $\\omega_i$ 를 선택하는 결정을 $\\alpha_i$ 라고 정의하고, 특징 $x$ 를 클래스 $\\omega_i$\n",
    "로 매핑하는 전체 결정규칙을 $\\alpha(x)$ 라고 정의한다. 즉, $\\alpha(x) \\rightarrow \\{ \\alpha_1, \\alpha_2, …, \\alpha_c \\}$. 특징 $x$ 를 클래스  $\\omega_i$  로 할당하는 (조건적) 베이즈 위험 $ R(\\alpha_i|x)$ 은 다음과 같이 표현된다.\n",
    "\n",
    "$$\\mathscr{R}(\\alpha(x) \\rightarrow \\alpha_i) = \\mathscr{R}(\\alpha_i |x) = \\sum_{j=1}^C C_{ij} P (\\omega_j | x) $$\n",
    "\n",
    "그리고 전체 결정 규칙 $\\alpha(x)$과 관련된 베이즈 위험은 다음과 같이 표현된다.\n",
    "\n",
    "$$\\mathscr{R}(\\alpha(x)) = \\int \\mathscr{R}(\\alpha(x) |x) P(x) dx $$\n",
    "\n",
    "이 표현식이 최소화 되기 위해서는 특징공간 상의 각 점 $x$ 에서 베이즈 위험$ R(\\alpha (x)|x)$ 이 최소화 되어야 한다. 즉, $ R(\\alpha_i|x)$ 이 최소인 $\\omega_i$ 를 선택하는 것과 같다.\n",
    "\n",
    "<img src=\"images/multi-bayes-risk.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 판별함수\n",
    "\n",
    "이 장에서 설명한 모든 결정 규칙들은 동일한 구조를 갖고 있다. 이러한 구조는 분류기준함수의 집합 $ g_i(x), i=1,\\cdots,x$ 에 대하여 다음과 같이 정리될 수 있다.\n",
    "\n",
    "만약, $g_i(x) > g_j(x) \\quad \\forall j\\ne i$ 이라면, **특징벡터 $x$를 클래스 $\\omega_i$** 에 속한다고 결정. \n",
    "\n",
    "따라서, $C$ 개의 클래스 중의 하나로 결정하는 시스템은 $C$ 개의 판별함수로 구성된 네트워크로 표현하는 가장 큰 값을 출력하는 카테고리를 선택하는 구조를 가진다.\n",
    "\n",
    "<img src=\"images/discr-func.png\" width=450>\n",
    "\n",
    "\n",
    "| criterion | Discriminant Function |\n",
    "|-----------|-----------------------|\n",
    "| Bayes     | $g_i(x) = -\\mathscr{R}(\\alpha_i | x)$ |\n",
    "| MAP       | $g_i(x) = P(\\omega_i | x)$ |\n",
    "| ML        | $g_i(x) = P(x | \\omega_i) $ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최우추정법에 의한 확률밀도함수의 추정\n",
    "\n",
    "지금까지는 **확률밀도가 주어진 경우**에 영역 결정과 분류기를 어떻게 만드는 지를 설명하였다. 하지만, 대부분의 경우 **실제 확률밀도분포에 대한 지식은 입수 할 수 없으며**, 실험자료로부터 결정되어야 한다. 이 때에는 두 가지 접근법이 일반적이다.\n",
    "\n",
    "1. **파라미타 추정 (Parameter Estimation)법** : 밀도에 대하여 특정한 형태를 가정하여 (e.g.가우시안) 이를 결정짖는 파라미터들 (e.g. 평균과 분산)을 최대우도추정 (Maximum Likelihood Estimation) 방법을 통해서 결정한다.\n",
    "\n",
    "2. **비모수 밀도 추정 (Non-parametric Density Estimation)법** : 밀도에 대하여 어떠한 지식도 가정하지 않는 방법으로, 커널밀도 추정법 또는 최근접이웃 규칙법 등이 있다. \n",
    "\n",
    "**최우추정 (Maximum Likelihood Estimation:MLE)** 이란?: \n",
    "\n",
    "“주어진 자료 $X$를 발생시켰을 가능성이 가장 높은 매개 변수 $\\theta$를 찾아라.”\n",
    "\n",
    "즉, 주어진 자료 $X$에 대해 가장 큰 우도를 갖는 $\\theta$를 찾는 것과 같다.  아래의 그림과 같은 예에서 : \n",
    "\n",
    "$$P(X| \\theta_1) > P(X|\\theta_2)$$\n",
    " \n",
    " \n",
    "<img src=\"./images/ML-1.png\" width=\"300\">\n",
    "\n",
    "최대 우도를 갖는 $\\theta$ 는? 파라미터들이 고정되어 있으나 모른다고 가정한다. 최우추정 해는 다음과 같이 자료 $X$를 가장 잘 설명하는 해를 구한다.\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[p(X|\\theta)] $$\n",
    " \n",
    "<img src=\"./images/ML-2.jpg\" width=\"250\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**최대 우도 추정 (최우추정, MLE : Maximum Likelihood Estimation)**\n",
    "\n",
    "일련의 파라미터들 $\\theta=[\\theta_1, \\theta_2, …, \\theta_N]^T$ 으로 구성된 어떤 확률밀도함수 $p(x|\\theta)$ 로부터 관측된 표본 데이터 집합을 $X={x^{(1},x^{(2},…,x^{(N}}$ 라 할 때, \n",
    "이 표본들로부터 파라미터 $\\theta=[\\theta_1, \\theta_2, …, \\theta_N]^T$  들을 추정하는 문제를 생각해보자. (즉, 평균, $\\mu_0$, 와 분산,$\\sigma_0^2$ 를 갖는 가우시안 확률분포로부터 관측된 표본데이터로부터 평균과 분산을 추정하는 문제). 어떤 표본 집합이 특정한 확률밀도 함수 $p(x|\\theta)$ 로 표현되는 프로세스로부터 발생한 데이터로 이루어져 있다면, 전체 표본 집합은 결합확률밀도로 다음과 같이 표현된다.\n",
    "\n",
    "$$ p(X|\\theta) = p(x^{(1} | \\theta) p(x^{(2} | \\theta) \\cdots p(x^{(N} | \\theta) = \\prod_{k=1}^N p(x^{(k} | \\theta) $$\n",
    "\n",
    "그런데, 이 함수는 확률함수이므로 가장 큰 확률 값을 발생시키는 $\\theta$ 값을 추정값  $\\hat{\\theta}$    으로 보는 것이 가장 그럴듯할 것이다. 이는 자료를 가장 그렇게 줄 수 있는 θ 값을 얻는 직관적인 생각에 해당한다. 여기서, $p(x|\\theta)$ 는 파라미터 $\\theta$  에 따르는 주어진 데이터 집합의 우도(likelihood) 함수이다. 우도 함수의 곱을 합으로 하기 위해서 로그를 취하면 다음과 같이 된다.\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[p(X|\\theta)] = \\arg\\max[\\log p(X|\\theta)]$$\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[\\log \\prod_{k-1}^N p(x^{(k}|\\theta)] = \\arg\\max[ \\sum_{k-1}^N \\log p(x^{(k}|\\theta)]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**로그-우도를 사용하는 이유**\n",
    "\n",
    "분석을 위해서는 우도의 로그를 갖고 일하는 것이 편리하다. 로그는 단조함수(monotonic function: 단조증가함수와 단조감소함수의 총칭 ) 이므로\n",
    "\n",
    "<img src=\"images/log-ML.png\" width=\"550\">\n",
    "\n",
    "로그를 취하면 파라미터의 최우추정은 다음과 같이 되어\n",
    "\n",
    "$$ \\hat{\\theta} = \\arg\\max[\\log \\prod_{k-1}^N p(x^{(k}|\\theta)] = \\arg\\max[ \\sum_{k-1}^N \\log p(x^{(k}|\\theta)]  $$\n",
    "\n",
    "\n",
    "* 항들의 합을 최대화 시키는 것은 곱을 최대화 시키는 것보다 쉬운 일이며  (항들의 곱들에 대한 미분을 생각해 보면 알 수 있음), \n",
    "* 분포가 가우시안이면 로그를 취하는 것의 장점이 더 명확해진다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**단변량 가우시안의 경우**: \n",
    "\n",
    "확률밀도함수가 $p(x) = \\mathscr{N}(\\mu, \\sigma)$ 로 주어진 자료 $X=\\{x^{(1},x^{(2},…,x^{(N}\\}$ 에서 표준편차 $\\sigma$ 가 주어진 경우, 평균의 최우추정은?\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{\\theta} & = \\arg\\max \\sum_{k=1}^N \\log p(x^{(k} | \\theta) \\\\\n",
    "             & = \\arg\\max \\sum_{k=1}^N \\log \\left( {1 \\over \\sqrt{2\\pi} \\sigma} \\exp \\left( - {1 \\over 2 \\sigma^2} (x^{(k} - \\mu)^2 \\right)  \\right) \\\\\n",
    "             & = \\arg\\max \\sum_{k=1}^N \\left( \\log \\left( {1 \\over \\sqrt{2\\pi} \\sigma} \\right) - {1 \\over 2 \\sigma^2} (x^{(k} - \\mu)^2  \\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "함수의 최대(또는 최소)는 미분이 0 이 되는 곳으로 정의되므로,\n",
    "\n",
    "$$ {\\partial \\over \\partial \\mu} \\sum_{k=1}^N \\left( \\log \\left( {1 \\over \\sqrt{2\\pi} \\sigma} \\right) - {1 \\over 2 \\sigma^2} (x^{(k} - \\mu)^2 \\right)$$\n",
    "\n",
    "$$ = - {1 \\over 2 \\sigma^2} {\\partial \\over \\partial \\mu} \\sum_{k=1}^N \\left( x^{2(k} -2x^{(k}\\mu + \\mu^2 \\right)  = {1 \\over 2 \\sigma^2} {\\partial \\over \\partial \\mu} \\sum_{k=1}^N \\left( 2x^{(k}\\mu - \\mu^2 \\right) = 0 $$\n",
    "$$ \\Rightarrow  \\sum_{k=1}^N \\left( x^{(k} - \\mu \\right) = \\sum_{k=1}^N x^{(k} -N \\mu = 0 \\Rightarrow \\mu = {1 \\over N} \\sum_{k=1}^N x^{(k}  $$\n",
    "\n",
    "따라서 **가우시안 확률밀도의 평균의 최우추정은 훈련자료의 산술 평균**(매우 직관적인 결과)이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률밀도함수가 $ p(x) = \\mathscr{N}(\\mu, \\sigma)$ 로 주어진 자료  $X=\\{x^{(1},x^{(2},…,x^{(N}\\}$  에서 표준편차(σ) 와 평균(μ)의 최우추정은 변수가 두 개이므로 미분은 그래디언트가 된다.\n",
    "\n",
    "$$ \\hat{\\theta} = \\begin{bmatrix} \\theta_1 = \\mu \\\\ \\theta_2 = \\sigma^2 \\end{bmatrix}\n",
    "\\Rightarrow \\nabla_\\theta = \\begin{bmatrix} {\\partial \\over \\partial \\theta_1} \\sum_{k=1}^N \\log p(x^{(k} | \\theta) \\\\  {\\partial \\over \\partial \\theta_2} \\sum_{k=1}^N \\log p(x^{(k} | \\theta) \\end{bmatrix}\n",
    "= \\sum_{k=1}^N \\begin{bmatrix} {1 \\over \\theta_2}(x^{(k} - \\theta_1)  \\\\ -{1 \\over 2\\theta_2} + {(x^{(k} - \\theta_1)^2 \\over 2 \\theta_2^2 } \\end{bmatrix} = 0 $$\n",
    "\n",
    "\n",
    "$\\theta_1$ 과 $\\theta_2$ 에 대하여 풀면\n",
    "\n",
    "\n",
    "$$ \\hat{\\theta}_1 = { 1 \\over N} \\sum_{k=1}^N x^{(k};  \\quad \\hat{\\theta}_2 = { 1 \\over N} \\sum_{k=1}^N (x^{(k} - \\hat{\\theta}_1)^2$$\n",
    "\n",
    "따라서 **분산의 최우추정은 훈련자료의 표본분산**이 되며, \n",
    "\n",
    "**다변량 가우시안의 최우추정**은 **표본평균벡터**와 **표본공분산행렬**이 됨도 유사한 방법으로 보일 수 있다. \n",
    "\n",
    "\n",
    "$$ \\hat{\\mathbf{\\mu}} = { 1 \\over N} \\sum_{k=1}^N \\mathbf{x}^{(k};  \\quad \\hat{\\Sigma} = { 1 \\over N} \\sum_{k=1}^N (\\mathbf{x}^{(k} - \\hat{\\mu}) (\\mathbf{x}^{(k} - \\hat{\\mu})^T$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최우추정법에 의한 확률밀도함수의 추정\n",
    "\n",
    "이러한 추정이 얼마나 잘 된 것인지를 어떻게 알 수 있나? 통계학적 추정의 정확도를 말해주는 두 측정치 bias 와 Variance\n",
    "\n",
    "* Bias : 추정치가 실제 값에 얼마나 가까운가를 말하는 정도\n",
    "* Variance : 자료가 다를 경우 추정치가 얼마나 변화하는가를 나타내는 수치.\n",
    "\n",
    "<img src=\"images/bias-variance.png\" width=\"250\">\n",
    "\n",
    "Bias-Variance tradeoff : 하나를 감소시키기 위해서는 다른 하나를 희생해야만 함\n",
    "\n",
    "<img src=\"images/bias-variance-2.png\" width=\"450\">\n",
    "\n",
    "* **평균의 최우추정은 언바이어스드(unbiased) 추정**이다.!\n",
    "\n",
    "$$ E[\\hat{\\mu}] = E \\left[ { 1 \\over N} \\sum_{k=1}^N x_k \\right] = { 1 \\over N} \\sum_{k=1}^N E[x_k] = \\mu$$\n",
    "\n",
    "* **분산의 최우추정은 바이어스드(biased) 추정**이다. !\n",
    "\n",
    "$$ E[\\hat{\\sigma}^2] = E \\left[ { 1 \\over N} \\sum_{k=1}^N (x_k -\\hat{\\mu})^2 \\right] = \\cdots =\n",
    "{ N-1 \\over N} Var(x_k) \\ne Var(x_k)$$\n",
    "\n",
    "이는 분산의 최우추정에 진짜 평균이 아닌 최우추정 평균이 사용되기 때문이다.\n",
    "하지만 $N$ 이 무한이 증가하면, 바이어스는 제로(0)에 가까워진다. \n",
    "따라서 때때로 다음과 같은 언바이어스드 공분산을 사용한다. **다변량의 경우**에는 다음과 같이 된다.\n",
    "\n",
    "$$  \\hat{\\Sigma}_{\\text{unbiased}} = { 1 \\over N-1} \\sum_{k=1}^N (x^{(k} - \\hat{\\mu}) (x^{(k} - \\hat{\\mu})^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 최우추정과 우도비검증 판별함수 시뮬레이션\n",
    "\n",
    "가우시안 분포를 이루는 데이터에 대하여 **최우추정(MLE)법**으로 우도함수의 파라미터를 추정하고, **우도비검증(LRT) 판별함수**를 이용하여 미지의 패턴을 인식하는 실습을 해보자.\n",
    "\n",
    "1. 가우시안 분포로 2-클래스를 이루는 임의의 데이터 집합을 생성한다.\n",
    "2. 주어진 데이터에서 확률밀도함수를 MLE법으로 추정한다.\n",
    "3. 추정된 확률밀도함수가 원래 확률밀도함수에 얼마나 근접한지를 확인한다.\n",
    "4. 임의의 데이터가 주어질 경우에, 이 데이터가 어느 클레스에 속하는지를 결정한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[380, 400] [380.68565937 400.08691714]\n",
      "[[200, 0], [0, 200]] [[242.91808231  12.9359439 ]\n",
      " [ 12.9359439  177.61844631]]\n",
      "[430, 350] [428.22170285 348.53371804]\n",
      "[[240, 150], [150, 240]] [[225.80569891 118.22755801]\n",
      " [118.22755801 196.21795574]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mu1 = [380, 400]\n",
    "cov1 = [[200, 0],[0,200]]\n",
    "rv1 = sp.stats.multivariate_normal(mu1,cov1)\n",
    "X1 = rv1.rvs(200)\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(X1[:,0],X1[:,1],s=1)\n",
    "\n",
    "mu2= [430,350]\n",
    "cov2 = [[240, 150], [150, 240]]\n",
    "rv2 = sp.stats.multivariate_normal(mu2, cov2)\n",
    "X2 = rv2.rvs(200)\n",
    "\n",
    "plt.scatter(X2[:,0], X2[:,1],s=2)\n",
    "\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "\n",
    "mu1_ = X1.mean(axis=0)\n",
    "cov1_ = np.cov(X1.T, ddof=0)\n",
    "print(mu1, mu1_)\n",
    "print(cov1, cov1_)\n",
    "plt.scatter(mu1[0], mu1[1],marker='o',s=60)\n",
    "plt.scatter(mu1_[0], mu1_[1], marker='x',s=60)\n",
    "\n",
    "mu2_ = X2.mean(axis=0)\n",
    "cov2_ = np.cov(X2.T, ddof=0)\n",
    "print(mu2, mu2_)\n",
    "print(cov2, cov2_)\n",
    "plt.scatter(mu2[0], mu2[1], marker='o',s=60)\n",
    "plt.scatter(mu2_[0], mu2_[1], marker='x',s=60)\n",
    "\n",
    "test_1 = [390, 380]\n",
    "plt.scatter(test_1[0], test_1[1], s=40, marker='^')\n",
    "test_2 = [420, 360]\n",
    "plt.scatter(test_2[0], test_2[1], s=40, marker='^')\n",
    "test_3 = [430, 370]\n",
    "plt.scatter(test_3[0], test_3[1], s=40, marker='^')\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[390, 380]  is  class_1\n",
      "[420, 360]  is  class_2\n",
      "[430, 370]  is  class_2\n"
     ]
    }
   ],
   "source": [
    "result_1_1 = sp.stats.multivariate_normal.pdf(test_1,mu1_,cov1_)\n",
    "result_1_2 = sp.stats.multivariate_normal.pdf(test_1,mu2_,cov2_)\n",
    "decision = \"class_1\" if result_1_1 > result_1_2 else \"class_2\"\n",
    "print( test_1, \" is \", decision)\n",
    "result_2_1 = sp.stats.multivariate_normal.pdf(test_2,mu1_,cov1_)\n",
    "result_2_2 = sp.stats.multivariate_normal.pdf(test_2,mu2_,cov2_)\n",
    "decision = \"class_1\" if result_2_1 > result_2_2 else \"class_2\"\n",
    "print( test_2, \" is \", decision)\n",
    "result_3_1 = sp.stats.multivariate_normal.pdf(test_3,mu1_,cov1_)\n",
    "result_3_2 = sp.stats.multivariate_normal.pdf(test_3,mu2_,cov2_)\n",
    "decision = \"class_1\" if result_3_1 > result_3_2 else \"class_2\"\n",
    "print( test_3, \" is \", decision)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
